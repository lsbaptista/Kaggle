{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2025 Google LLC.","metadata":{}},{"cell_type":"code","source":"# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-04T04:53:17.924118Z","iopub.execute_input":"2025-04-04T04:53:17.924587Z","iopub.status.idle":"2025-04-04T04:53:17.950628Z","shell.execute_reply.started":"2025-04-04T04:53:17.924547Z","shell.execute_reply":"2025-04-04T04:53:17.94966Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bonus Content!\n\nCongrats on finishing the 5-day Generative AI Intensive course from Kaggle and Google!\n\nThis notebook is a \"bonus episode\" that highlights a few more things you can do with the Gemini API that weren't covered during the course. This material doesn't pair with the whitepapers or podcast, but covers some extra features that you might find useful when building Gemini API powered apps.","metadata":{}},{"cell_type":"markdown","source":"## Get set up\n\nInstall the SDK and other tools for this notebook, then import the package and set up a retry policy so you don't have to manually retry when you hit a quota limit.","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qyy jupyterlab\n!pip install -qU \"google-genai==1.9.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:53:17.952889Z","iopub.execute_input":"2025-04-04T04:53:17.953283Z","iopub.status.idle":"2025-04-04T04:53:36.295763Z","shell.execute_reply.started":"2025-04-04T04:53:17.953251Z","shell.execute_reply":"2025-04-04T04:53:36.294272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\nfrom IPython.display import display, Image, Markdown, Audio\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:53:36.29739Z","iopub.execute_input":"2025-04-04T04:53:36.297754Z","iopub.status.idle":"2025-04-04T04:53:37.661407Z","shell.execute_reply.started":"2025-04-04T04:53:36.297718Z","shell.execute_reply":"2025-04-04T04:53:37.660347Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Set up your API key\n\nTo run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n\nIf you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n\nTo make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:53:37.663561Z","iopub.execute_input":"2025-04-04T04:53:37.66402Z","iopub.status.idle":"2025-04-04T04:53:38.003282Z","shell.execute_reply.started":"2025-04-04T04:53:37.663987Z","shell.execute_reply":"2025-04-04T04:53:38.002318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n\n![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)","metadata":{}},{"cell_type":"markdown","source":"### Automated retry\n\nSet up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:53:38.004457Z","iopub.execute_input":"2025-04-04T04:53:38.004738Z","iopub.status.idle":"2025-04-04T04:53:38.176651Z","shell.execute_reply.started":"2025-04-04T04:53:38.004711Z","shell.execute_reply":"2025-04-04T04:53:38.175774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Multi-modal prompting\n\nAs you may have noticed in AI Studio, the Gemini models support more than just text as input. You can provide pictures, videos, audio and more.\n\n\n### Images\n\nStart by downloading an image.","metadata":{}},{"cell_type":"code","source":"import PIL\n\n!wget -nv https://storage.googleapis.com/generativeai-downloads/images/cake.jpg\nImage('cake.jpg', width=500)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:53:38.177815Z","iopub.execute_input":"2025-04-04T04:53:38.178291Z","iopub.status.idle":"2025-04-04T04:53:39.595763Z","shell.execute_reply.started":"2025-04-04T04:53:38.178257Z","shell.execute_reply":"2025-04-04T04:53:39.594419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Python SDK can take a list as the prompt input. This list represents a sequence of prompt parts, and while each part needs to be a single mode (such as text or image), you can combine them together to form a multi-modal prompt.","metadata":{}},{"cell_type":"code","source":"prompt = [\n  \"What is this? Please describe it in detail.\",\n  PIL.Image.open(\"cake.jpg\"),\n]\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt\n)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:53:39.597884Z","iopub.execute_input":"2025-04-04T04:53:39.598364Z","iopub.status.idle":"2025-04-04T04:53:42.41863Z","shell.execute_reply.started":"2025-04-04T04:53:39.598311Z","shell.execute_reply":"2025-04-04T04:53:42.417463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Image understanding in the Gemini models can be quite powerful. Check out [this guide on object detection](https://github.com/google-gemini/cookbook/blob/main/examples/Object_detection.ipynb), where the Gemini API identifies and highlights objects in an image based on a prompt.\n\nMore input modes are supported, but first take a look at how to handle large files.","metadata":{}},{"cell_type":"markdown","source":"## Use and upload files\n\nThe Gemini models have very large context windows, most have at least 1 million tokens, and some have up to 2M input tokens! This translates to up to 2 hours of video or up to 19 hours of audio.\n\nAs files of this length are typically too large to send in HTTP requests, the Gemini API provides a File API to that you can use to send large files in requests. It also means you can reuse the same files across different requests without having to re-upload the same content each time, improving your request latency.\n\nNote that some file limits exist, including how long they are kept. See [the vision docs](https://ai.google.dev/gemini-api/docs/vision?hl=en&lang=python) for more info.","metadata":{}},{"cell_type":"markdown","source":"### Audio\n\nThe Gemini API supports audio as an input medium. If you are the kind of person that takes audio notes with the Recorder or Voice Memo apps, this can be an efficient way to interact with your recordings ([check out this example](https://github.com/google-gemini/cookbook/blob/main/examples/Voice_memos.ipynb)), but you are not limited to personal notes.\n\nThis MP3 audio recording is a State of the Union addess from US president Kennedy. Running the following code should give you a playable audio controller so you can listen to it.","metadata":{}},{"cell_type":"code","source":"from pydub import AudioSegment\nfrom IPython.display import Audio\n\n\n!wget -nv https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3 -O speech.mp3\n\n# This audio file is over 40mb, so trim the file before sending it to your browser.\nfull_speech = AudioSegment.from_mp3(\"speech.mp3\")\n\n# Preview the first 30 seconds.\nfirst_30s_speech = full_speech[:30000]\nfirst_30s_speech\n\n# If you want to download and listen to the whole file, uncomment this.\n# Audio(\"speech.mp3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:53:42.419799Z","iopub.execute_input":"2025-04-04T04:53:42.420105Z","iopub.status.idle":"2025-04-04T04:53:53.969816Z","shell.execute_reply.started":"2025-04-04T04:53:42.420075Z","shell.execute_reply":"2025-04-04T04:53:53.968692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now upload the full file so it can be used in a prompt.","metadata":{}},{"cell_type":"code","source":"uploaded_speech = client.files.upload(file='speech.mp3')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:53:53.971702Z","iopub.execute_input":"2025-04-04T04:53:53.972649Z","iopub.status.idle":"2025-04-04T04:53:55.203689Z","shell.execute_reply.started":"2025-04-04T04:53:53.972597Z","shell.execute_reply":"2025-04-04T04:53:55.202613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"Who made the following speech? What were they positive about?\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=[prompt, uploaded_speech]\n)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:53:55.208318Z","iopub.execute_input":"2025-04-04T04:53:55.208699Z","iopub.status.idle":"2025-04-04T04:54:08.864848Z","shell.execute_reply.started":"2025-04-04T04:53:55.208666Z","shell.execute_reply":"2025-04-04T04:54:08.863813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Video\n\n","metadata":{}},{"cell_type":"markdown","source":"Now try out video understanding. In this example you will upload the [\"Big Buck Bunny\"](https://peach.blender.org/) short film and use the Gemini API to ask questions.\n\n> \"Big Buck Bunny\" is (c) copyright 2008, Blender Foundation / www.bigbuckbunny.org and [licensed](https://peach.blender.org/about/) under the [Creative Commons Attribution 3.0](http://creativecommons.org/licenses/by/3.0/) License.\n\nStart by downloading the video to this notebook and then uploading to the File API.","metadata":{}},{"cell_type":"code","source":"!wget -nv https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4\n\nprint(\"Uploading to the File API...\")\nvideo_file = client.files.upload(file=\"BigBuckBunny_320x180.mp4\")\nprint(\"Upload complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:08.866098Z","iopub.execute_input":"2025-04-04T04:54:08.866407Z","iopub.status.idle":"2025-04-04T04:54:11.950781Z","shell.execute_reply.started":"2025-04-04T04:54:08.866371Z","shell.execute_reply":"2025-04-04T04:54:11.949542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Larger files can take some time to process when they upload. Ensure that the file is ready to use.","metadata":{}},{"cell_type":"code","source":"import time\n\nwhile video_file.state.name == \"PROCESSING\":\n    print('Waiting for video to be processed.')\n    time.sleep(10)\n    video_file = client.files.get(name=video_file.name)\n\nif video_file.state.name == \"FAILED\":\n  raise ValueError(video_file.state.name)\n\nprint(f'Video processing complete: ' + video_file.uri)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:11.952682Z","iopub.execute_input":"2025-04-04T04:54:11.953026Z","iopub.status.idle":"2025-04-04T04:54:22.278635Z","shell.execute_reply.started":"2025-04-04T04:54:11.952992Z","shell.execute_reply":"2025-04-04T04:54:22.277524Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that it is ready, use it in a prompt. Note that using large files in requests typically takes more time than a small text request, so increase the timeout and be aware that you may have to wait for this response.","metadata":{}},{"cell_type":"code","source":"prompt = \"What characters are in this movie?\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=[prompt, video_file]\n)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:22.280083Z","iopub.execute_input":"2025-04-04T04:54:22.281007Z","iopub.status.idle":"2025-04-04T04:54:32.710902Z","shell.execute_reply.started":"2025-04-04T04:54:22.280959Z","shell.execute_reply":"2025-04-04T04:54:32.709836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Streaming\n\nSo far, you have been making transactional requests with the Gemini API - send the request, receive a full response. The API also supports response streaming.\n\nUse `generate_content_stream` instead of `generate_content` to trigger streaming mode. Note that it may render quickly - uncomment the final `print` to see each streamed chunk on its own.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"Write an essay defending why dogs are the best animals.\nTreat the essay as serious and include proper essay structure.\"\"\"\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash',\n    contents=prompt\n)\n\nfor chunk in response:\n    print(chunk.text, end='')\n    time.sleep(0.3)  # Slow down to show streaming\n\n    # Uncomment this to see the individual tokens in separate sections.\n    # print(\"\\n----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:32.712423Z","iopub.execute_input":"2025-04-04T04:54:32.712755Z","iopub.status.idle":"2025-04-04T04:54:37.336143Z","shell.execute_reply.started":"2025-04-04T04:54:32.712722Z","shell.execute_reply":"2025-04-04T04:54:37.335197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using the Live API\n\nStarting with Gemini 2.0, you can interact through a Live API that supports bi-directional streaming of text or audio, and you can also provide a video input stream.\n\nThis mode provides a powerful way to talk live to the Gemini API in a natural way. Notebooks don't easily support streaming audio as input, but read ahead to see how you can stream text in and audio out.\n\nWhen you're done you should head over to [AI Studio](https://aistudio.google.com/live) to try a live session. Try sharing your screen with the model in an audio chat to get the model to help you with tasks on your computer. It can help you write code or answer questions about whatever is on the screen. Gemini, if you're reading this, say hi!\n\n![Gemini Live API with screen sharing](https://storage.googleapis.com/generativeai-downloads/kaggle/live-screen-share.png)","metadata":{}},{"cell_type":"markdown","source":"First define some configuration.","metadata":{}},{"cell_type":"code","source":"live_client = genai.Client(\n    api_key=GOOGLE_API_KEY,\n    http_options=types.HttpOptions(api_version='v1alpha'),\n)\n\n# For the Live API, you need to use the 2.0 experimental model and v1alpha API.\nMODEL = 'gemini-2.0-flash-exp'\n\n# Set up an audio-out session.\nCONFIG = types.LiveConnectConfig(\n    response_modalities=[\"AUDIO\"],\n    speech_config=types.SpeechConfig(\n        voice_config=types.VoiceConfig(\n\n            # Try changing the voice! Pick one of Puck, Charon, Kore, Fenrir, Aoede\n            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name=\"Kore\")\n        )\n    )\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:37.337333Z","iopub.execute_input":"2025-04-04T04:54:37.337633Z","iopub.status.idle":"2025-04-04T04:54:37.369873Z","shell.execute_reply.started":"2025-04-04T04:54:37.337604Z","shell.execute_reply":"2025-04-04T04:54:37.368903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define a helper for buffering audio responses.","metadata":{}},{"cell_type":"code","source":"import contextlib\nimport wave\n\n@contextlib.contextmanager\ndef wave_file(filename, channels=1, rate=24000, sample_width=2):\n  \"\"\"Context managed to buffer audio into a wave file with suitable headers.\"\"\"\n  with wave.open(filename, \"wb\") as wf:\n    wf.setnchannels(channels)\n    wf.setsampwidth(sample_width)\n    wf.setframerate(rate)\n    yield wf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:37.37105Z","iopub.execute_input":"2025-04-04T04:54:37.371376Z","iopub.status.idle":"2025-04-04T04:54:37.377339Z","shell.execute_reply.started":"2025-04-04T04:54:37.371345Z","shell.execute_reply":"2025-04-04T04:54:37.376222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now start the session. You may recall from the course that as the Live API requires real-time interaction, we need to set up the conversation ahead of time. In this example you have a single line of dialog pre-scripted.","metadata":{}},{"cell_type":"code","source":"async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n\n    message = \"Hi there, can you tell me something fun about spiders?\"\n    print('>', message)\n\n    # Send the message to the model.\n    await session.send(input=message, end_of_turn=True)\n\n    # Set up a temporary audio file to store the audio response.\n    with wave_file(file_name := \"audio_chat.wav\") as wav:\n\n      # Start receiving and handling the response\n      async for chunk in session.receive():\n        # Text responses.\n        if chunk.text is not None:\n          print(chunk.text, end='')\n\n        # Audio responses.\n        elif chunk.data is not None:\n          wav.writeframes(chunk.data)\n          print('.', end='')\n\n    display(Audio(file_name, autoplay=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:55:24.442339Z","iopub.execute_input":"2025-04-04T04:55:24.44311Z","iopub.status.idle":"2025-04-04T04:55:34.015641Z","shell.execute_reply.started":"2025-04-04T04:55:24.443067Z","shell.execute_reply":"2025-04-04T04:55:34.014598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now you can try interacting in real time! As this example requires you to type your input live, it is commented out. Be sure to uncomment the last line to run the example!","metadata":{}},{"cell_type":"code","source":"async def start_chat_with_user_input():\n    async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n\n      print('Starting text-in, audio-out chat! Type \"q\" to quit.')\n      while (message := input('> ')).lower()[0] != 'q':\n    \n        # Send the message to the model.\n        await session.send(input=message, end_of_turn=True)\n    \n        # Set up a temporary audio file to store the audio response.\n        with wave_file(file_name := \"audio_chat.wav\") as wav:\n    \n          # Start receiving and handling the response\n          async for chunk in session.receive():\n            # Text responses.\n            if chunk.text is not None:\n              print(chunk.text, end='')\n    \n            # Audio responses.\n            elif chunk.data is not None:\n              wav.writeframes(chunk.data)\n              print('.', end='')\n    \n        display(Audio(file_name, autoplay=True))\n\n# Uncomment this to run the live chat. 'q' will end the conversation.\n# await start_chat_with_user_input()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:47.416267Z","iopub.execute_input":"2025-04-04T04:54:47.41659Z","iopub.status.idle":"2025-04-04T04:54:47.424435Z","shell.execute_reply.started":"2025-04-04T04:54:47.416557Z","shell.execute_reply":"2025-04-04T04:54:47.423263Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Live API can do more than just chat too, you can use code generation/execution, add tools like Google Search as well as bring your own custom tools, like you did on day 3.  For more examples check out the [tools](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb) and [plotting](https://github.com/google-gemini/cookbook/blob/main/examples/LiveAPI_plotting_and_mapping.ipynb) guides in the cookbook.","metadata":{}},{"cell_type":"markdown","source":"## Image generation\n\nNew in the Gemini API is the ability to generate images. Try out ","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"Can you create a 3d rendered image of a pig with wings\nand a top hat flying over a happy futuristic scifi city with lots\nof greenery?\n\"\"\"\n\nresponse = client.models.generate_content(\n    # Use the dedicated image generation model.\n    model=\"gemini-2.0-flash-exp-image-generation\",\n    contents=prompt,\n    # This model requires both text and image outputs.\n    config=types.GenerateContentConfig(\n      response_modalities=['text', 'image']\n    )\n)\n\nfor part in response.candidates[0].content.parts:\n\n  if part.text:\n    print(part.text)\n\n  elif part.inline_data:\n    display(Image(part.inline_data.data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:47.425873Z","iopub.execute_input":"2025-04-04T04:54:47.426262Z","iopub.status.idle":"2025-04-04T04:54:51.183358Z","shell.execute_reply.started":"2025-04-04T04:54:47.426227Z","shell.execute_reply":"2025-04-04T04:54:51.182223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Image generation works in a chat conversation too, so you can ask the model follow-up questions and it can make edits to the image. You can also provide images as input, for the model to start from.\n\nYou can try writing this code yourself, but the easiest way to explore the feature is through [the image generation model in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-exp-image-generation).","metadata":{}},{"cell_type":"markdown","source":"## Context caching\n\nContext caching is a technique that allows you to cache part of a request, such that it does not need to be re-processed by the model each time you use it. This is useful, for example, for asking new questions of the same documents.\n\nNote that context caching typically charges per million tokens per hour of caching. If you are using a paid API key, be sure to set your cache expiry or delete the cached tokens after use. See the [billing page](https://ai.google.dev/pricing) for more info. The Flash 1.5 model also supports caching on the free tier.\n\nTo ensure that the cache remains valid, caches are created by specifying versioned model names, so `gemini-1.5-flash-001`, where `-001` signifies the model version.","metadata":{}},{"cell_type":"code","source":"# Download the transcript\n!wget -nv -O apollo11.txt https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n\n# Upload to the File API\ntranscript_file = client.files.upload(file='apollo11.txt')\n\n# Create a cache\napollo_cache = client.caches.create(\n    model='gemini-1.5-flash-001',\n    config=types.CreateCachedContentConfig(\n        system_instruction=\"You are a space history buff that enjoys discussing and explaining historical space events.\",\n        contents=[transcript_file],\n    ),\n)\n\napollo_cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:51.184591Z","iopub.execute_input":"2025-04-04T04:54:51.184902Z","iopub.status.idle":"2025-04-04T04:54:54.754652Z","shell.execute_reply.started":"2025-04-04T04:54:51.18487Z","shell.execute_reply":"2025-04-04T04:54:54.753392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now you can use this cache when generating content.","metadata":{}},{"cell_type":"code","source":"response = client.models.generate_content(\n    model=apollo_cache.model,\n    config=types.GenerateContentConfig(\n        cached_content=apollo_cache.name,\n    ),\n    contents=\"Find a nice moment from this transcript\"\n)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:54:54.756534Z","iopub.execute_input":"2025-04-04T04:54:54.757003Z","iopub.status.idle":"2025-04-04T04:55:05.114669Z","shell.execute_reply.started":"2025-04-04T04:54:54.756952Z","shell.execute_reply":"2025-04-04T04:55:05.113622Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The response object includes information about the number of tokens that were cached and otherwise used in the prompt.","metadata":{}},{"cell_type":"code","source":"response.usage_metadata.to_json_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:55:05.115911Z","iopub.execute_input":"2025-04-04T04:55:05.116248Z","iopub.status.idle":"2025-04-04T04:55:05.122953Z","shell.execute_reply.started":"2025-04-04T04:55:05.116216Z","shell.execute_reply":"2025-04-04T04:55:05.121899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And you can calculate how many non-cached tokens were used as input.","metadata":{}},{"cell_type":"code","source":"response.usage_metadata.total_token_count - response.usage_metadata.cached_content_token_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:55:05.124468Z","iopub.execute_input":"2025-04-04T04:55:05.124884Z","iopub.status.idle":"2025-04-04T04:55:05.137306Z","shell.execute_reply.started":"2025-04-04T04:55:05.124837Z","shell.execute_reply":"2025-04-04T04:55:05.136059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Delete the cache\n\nTo ensure you are not charged for any cached tokens you are not using, delete the cache. If you are on the free tier, you won't be charged, but it's good practice to clean up when you're done.","metadata":{}},{"cell_type":"code","source":"print(apollo_cache.name)\nclient.caches.delete(name=apollo_cache.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:55:05.13891Z","iopub.execute_input":"2025-04-04T04:55:05.13938Z","iopub.status.idle":"2025-04-04T04:55:05.321639Z","shell.execute_reply.started":"2025-04-04T04:55:05.139331Z","shell.execute_reply":"2025-04-04T04:55:05.320469Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Further reading\n\nTake a look through the [Gemini API cookbook](https://github.com/google-gemini/cookbook) for more feature-based quickstarts and complex examples. And don't forget to explore [AI Studio](https://aistudio.google.com) to try out API features directly in the browser.\n\nIf you enabled billing on your API key and are finished with the key, you can [turn it off](https://ai.google.dev/gemini-api/docs/billing) unless you plan on using it again.\n\nAnd **thank you** for coming with us on this 5-day learning journey!\n\n\\- [Mark McD](https://linktr.ee/markmcd)","metadata":{}}]}